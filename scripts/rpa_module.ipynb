{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_raw_text(raw_text):\n",
    "    soup = BeautifulSoup(raw_text, 'html.parser')\n",
    "    clean_soup = repr(soup.get_text().replace('\\n','.'))\n",
    "    clean_soup = re.sub(r'\\\\x[0-9A-Fa-f]{2}', ' ', clean_soup)\n",
    "    clean_soup = re.sub(r'\\\\u[0-9A-Fa-f]{4}', ' ', clean_soup)\n",
    "    clean_soup = re.sub(r'[\\.] +', '.', clean_soup)\n",
    "    clean_soup = re.sub(r'\\.+', '. ', clean_soup)\n",
    "    clean_soup = re.sub(' +', ' ', clean_soup)\n",
    "    return clean_soup\n",
    "\n",
    "def remove_entity_words(raw_text):\n",
    "    input_text = clean_raw_text(raw_text)\n",
    "    input_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_page_', input_text)\n",
    "    input_text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '_email_replaced_', input_text)\n",
    "    tags = nlp(input_text)\n",
    "    for entity in tags.ents:\n",
    "        tag = entity.label_\n",
    "        text = entity.text\n",
    "        if tag in ['PERSON', 'ORG', 'EVENT', 'GPE', 'NORP', 'PRODUCT', 'MONEY']:\n",
    "            input_text = input_text.replace(text, '_'+tag+'_')\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "import numpy as np\n",
    "\n",
    "def train_mlp(X,y):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000)  \n",
    "    mlp.fit(X, y) \n",
    "    return mlp\n",
    "    \n",
    "def train_svm(X, y):\n",
    "    svm = SVC(C=1000000.0, gamma=\"auto\", kernel='rbf')\n",
    "    svm.fit(X, y)\n",
    "    return svm\n",
    "\n",
    "def load_docs(csv_file,delimiter=','):\n",
    "    data = pd.read_csv(csv_file, delimiter=delimiter) \n",
    "    tags = np.array(data.is_order_email)\n",
    "    labels = np.array(data.label)\n",
    "    docs = []\n",
    "    for i in range(len(tags)):\n",
    "        docs.append((tags[i],remove_entity_words(labels[i])))\n",
    "#         docs.append((tags[i],clean_raw_text(labels[i])))\n",
    "    return docs\n",
    "\n",
    "def create_tfidf_training_data(docs,min_df=5):\n",
    "    # Create the training data class labels\n",
    "    y = [d[0] for d in docs]\n",
    "\n",
    "    # Create the document corpus list\n",
    "    corpus = [d[1] for d in docs]\n",
    "\n",
    "    # Create the TF-IDF vectoriser and transform the corpus\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, token_pattern=r'(?u)\\b\\w*[a-zA-Z]\\w*\\b')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X, y, vectorizer\n",
    "\n",
    "def tfidf_transform_data(docs, vectorizer):\n",
    "        # Create the training data class labels\n",
    "    y = [d[0] for d in docs]\n",
    "\n",
    "    # Create the document corpus list\n",
    "    corpus = [d[1] for d in docs]\n",
    "\n",
    "    # Create the TF-IDF vectoriser and transform the corpus\n",
    "    X = vectorizer.transform(corpus)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vectorizer = pickle.load(open(\"model/tfidf.pkl\", \"rb\"))\n",
    "model = pickle.load(open(\"model/svm_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(list_text, mark):\n",
    "    res = []\n",
    "    for text in list_text:\n",
    "        res.extend(text.split(mark))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_text_order_notif(raw_text):\n",
    "    raw_text = remove_entity_words(raw_text)\n",
    "    raws = [raw_text]\n",
    "#     raws = split(raws, '-')\n",
    "#     raws = split(raws, ',')\n",
    "#     raws = split(raws, '!')\n",
    "#     raws = split(raws, '?')\n",
    "#     raws = split(raws, '.')\n",
    "    print(raws)\n",
    "    test_words = []\n",
    "    for r in raws:\n",
    "        test_words.append((0,r))\n",
    "    x, y = tfidf_transform_data(test_words, vectorizer)\n",
    "    pred = model.predict(x)\n",
    "    print(pred)\n",
    "    for res in pred:\n",
    "        if res == 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Domino\\'s _ORG_ ', \" No Reply Domino's Pizza Online Ordering \", ' Email Confirmation Wed 6:01 _ORG_ _ORG_ dominos', ' com', ' sg Order Confirmation Dear MR RAYMOND DJAJALAKSANA Thank you for your order', ' To practise social distancing', ' we are now implementing Zero Contact Delivery and _GPE_', ' You have ordered the followi\"']\n",
      "[0 0 0 0 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = \"Google Security alert for your linked Google Account Sun 4/12 Your account ray-cap@hotmail.com is listed as the recovery email for raymond.djajalaksana@gmail.com. Don't recognize this account? Click here New sign-in to your linked account raymond.djajalaksana@gmail.com Your Google Account was just signed in to fr\"\n",
    "raw_text = \"Domino's Pizza Singapore - No Reply Domino's Pizza Online Ordering - Email Confirmation Wed 6:01 PM Domino's Pizza Singapore dominos.com.sg Order Confirmation Dear MR RAYMOND DJAJALAKSANA Thank you for your order! To practise social distancing, we are now implementing Zero Contact Delivery and Takeaway. You have ordered the followi\"\n",
    "\n",
    "is_text_order_notif(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NN / SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "base_path = 'raw_data/{}'\n",
    "docs = load_docs(base_path.format(\"order_search.csv\"))\n",
    "for additional_doc in load_docs(base_path.format(\"all_email_1.csv\"), delimiter='|'):\n",
    "    docs.append(additional_doc)\n",
    "for additional_doc in load_docs(base_path.format(\"all_email_2.csv\"), delimiter='|'):\n",
    "    docs.append(additional_doc)\n",
    "for additional_doc in load_docs(base_path.format(\"all_email_3.csv\"), delimiter='|'):\n",
    "    docs.append(additional_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, vector = create_tfidf_training_data(docs, min_df=7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9572953736654805\n",
      "[[244  10]\n",
      " [  2  25]]\n",
      "0.9644128113879004\n",
      "[[244   8]\n",
      " [  2  27]]\n"
     ]
    }
   ],
   "source": [
    "model = train_mlp(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(model.score(X_test, y_test))\n",
    "print(confusion_matrix(pred, y_test))\n",
    "\n",
    "model = train_svm(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(model.score(X_test, y_test))\n",
    "print(confusion_matrix(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n",
      "['00am', '00pm', '30am', '__org_', '_email_replaced_', '_event_', '_gpe_', '_money_', '_money__money_', '_norp_', '_org_', '_person_', '_product_', '_url_page_', 'a', 'about', 'above', 'access', 'account', 'add', 'added', 'address', 'after', 'alert', 'all', 'also', 'alumni', 'am', 'amazon', 'amount', 'an', 'and', 'announcement', 'another', 'any', 'anyone', 'app', 'apply', 'april', 'are', 'around', 'as', 'at', 'attachments', 'aug', 'august', 'automated', 'available', 'away', 'awesome', 'b', 'back', 'based', 'be', 'because', 'been', 'before', 'being', 'below', 'best', 'better', 'beverages', 'big_person_', 'bill', 'book', 'booking', 'break', 'browse', 'browser', 'bundle', 'by', 'call', 'can', 'cancelled', 'cap', 'card', 'career', 'catalog', 'cdjapan', 'celebrate', 'centre', 'change', 'changes', 'check', 'chicken', 'choosing', 'click', 'code', 'coffee', 'collection', 'com', 'combed', 'come', 'coming', 'complete', 'conditions', 'confirmation', 'confirmed', 'contact', 'contains', 'contract', 'course', 'coursera', 'courses', 'created', 'credit', 'credited', 'cup', 'curly', 'customer', 'd', 'daily', 'data', 'date', 'day', 'days', 'dbs', 'deal', 'deals', 'dear', 'dec', 'december', 'delicious', 'deliver', 'delivered', 'delivery', 'dependency', 'details', 'developer', 'development', 'did', 'dine', 'discussing', 'discussions', 'displayed', 'djajalaksana', 'do', 'domino', 'don', 'download', 'driver', 'due', 'during', 'e', 'early', 'east', 'easy', 'edit', 'email', 'emails', 'ends', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'ensure', 'even', 'event', 'every', 'everyone', 'exclusive', 'exp', 'experience', 'expire', 'expires', 'favourite', 'features', 'feb', 'february', 'feedback', 'few', 'find', 'first', 'flight', 'following', 'food', 'for', 'found', 'free', 'fri', 'friday', 'friends', 'from', 'full', 'fully', 'fun', 'funds', 'future', 'fw', 'game', 'get', 'github', 'give', 'glad', 'global', 'golden', 'good', 'got', 'grab', 'graphics', 'great', 'group', 'had', 'happening', 'happy', 'has', 'have', 'having', 'hear', 'hecalliot', 'hello', 'help', 'her', 'here', 'hey', 'hi', 'his', 'hit', 'home', 'hope', 'how', 'i', 'ibrahim', 'id', 'if', 'important', 'in', 'inform', 'information', 'instructed', 'instructor', 'interests', 'into', 'investments', 'invited', 'iqbal', 'is', 'issued', 'it', 'item', 'items', 'jan', 'january', 'job', 'jobs', 'join', 'jul', 'july', 'june', 'jurong', 'jushop_', 'just', 'k', 'keep', 'know', 'large', 'last', 'latest', 'lazmall', 'learn', 'learning', 'let', 'like', 'limited', 'line', 'link', 'list', 'listed', 'live', 'log', 'looking', 'love', 'ltd', 'm', 'made', 'maintain', 'make', 'manage', 'mar', 'march', 'match', 'matches', 'may', 'meal', 'member', 'members', 'message', 'might', 'minimum', 'miss', 'moment', 'mon', 'monday', 'money', 'more', 'morning', 'most', 'mr', 'much', 'muhammad', 'my', 'n', 'need', 'needs', 'net', 'never', 'new', 'news', 'next', 'no', 'not', 'note', 'notification', 'nov', 'now', 'number', 'o', 'ocbc', 'oct', 'of', 'off', 'offer', 'offers', 'on', 'one', 'online', 'only', 'or', 'order', 'ordered', 'ordering', 'orders', 'our', 'out', 'over', 'package', 'page', 'password', 'pay', 'payment', 'paynow', 'people', 'personal', 'pizza', 'pizzas', 'place', 'placed', 'plan', 'please', 'pm', 'points', 'policy', 'post', 'posted', 'posts', 'president', 'preview', 'price', 'pricing', 'prime', 'profile', 'promo', 'promotion', 'properly', 'provide', 'provided', 'pte', 'purchase', 'purchased', 'r', 'ray', 'raycap', 'raymond', 're', 'read', 'ready', 'receipt', 'receipts', 'receive', 'received', 'recent', 'recently', 'recognize', 'recommendations', 'recommended', 'redeem', 'redeemable', 'redemption', 'redmart', 'reference', 'registration', 'regular', 'reminder', 'rendez', 'rendez_vous', 'rendez_vous1', 'reply', 'repository', 'request', 'requested', 'required', 'respond', 'rewards', 'ride', 'right', 'rode', 's', 'safe', 'sale', 'saturday', 'save', 'savings', 'security', 'see', 'selected', 'seller', 'sender', 'sent', 'sep', 'september', 'service', 'services', 'set', 'sg', 'sgd', 'share', 'shipped', 'shipping', 'shopping', 'should', 'side', 'sides', 'sign', 'signed', 'singapore', 'sips', 'sized', 'so', 'some', 'something', 'sorry', 'special', 'specializations', 'specific', 'speed', 'starbucks', 'stars', 'start', 'started', 'stay', 'steam', 'step', 'still', 'stocks', 'store', 'stores', 'street', 'subject', 'successful', 'successfully', 'sunday', 'support', 'sure', 't', 'take', 'takeaway', 'tall', 'team', 'terms', 'thank', 'thanks', 'that', 'the', 'them', 'there', 'these', 'they', 'things', 'think', 'this', 'through', 'thu', 'thursday', 'time', 'to', 'today', 'top', 'total', 'track', 'tracking', 'transaction', 'transfer', 'transferred', 'travel', 'treat', 'trending', 'trip', 'trouble', 'try', 'tue', 'tuesday', 'twitter', 'two', 'type', 'u', 'udemy', 'unable', 'unread', 'until', 'up', 'update', 'updated', 'us', 'use', 'using', 'valid', 'valued', 've', 'vehicle', 'venti', 'verification', 'verify', 'version', 'via', 'view', 'viewing', 'visit', 'vouchers', 'vulnerable', 'wait', 'want', 'was', 'way', 'we', 'web', 'wed', 'wednesday', 'week', 'weekend', 'welcome', 'what', 'when', 'where', 'while', 'will', 'wishlist', 'with', 'withdrawn', 'working', 'world', 'would', 'write', 'www', 'x', 'year', 'you', 'your', 'yrs']\n"
     ]
    }
   ],
   "source": [
    "print(len(vector.get_feature_names()))\n",
    "print(vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('model/svm_model.pkl', 'wb'))\n",
    "pickle.dump(vector, open(\"model/tfidf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPA-Bot for email scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_account = ''\n",
    "email_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagui as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_into(xpath, type_cmd):\n",
    "    wait_element(xpath)\n",
    "    t.type(xpath, type_cmd)\n",
    "\n",
    "def click(xpath):\n",
    "    wait_element(xpath)\n",
    "    t.click(xpath)\n",
    "    \n",
    "def present(xpath):\n",
    "    return wait_element(xpath)\n",
    "\n",
    "def read(xpath):\n",
    "    wait_element(xpath)\n",
    "    return t.read(xpath)\n",
    "\n",
    "def wait_element(xpath):\n",
    "    for i in range(10):\n",
    "        if t.present(xpath):\n",
    "            return True\n",
    "        t.wait(1)\n",
    "    return False\n",
    "    \n",
    "def login_outlook(account, password):\n",
    "    t.url('https://login.live.com/login.srf?wa=wsignin1.0&rpsnv=13&ct=1586073207&rver=7.0.6737.0&wp=MBI_SSL&wreply=https%3a%2f%2foutlook.live.com%2fowa%2f%3fnlp%3d1%26RpsCsrfState%3d6590c65e-2e3f-b1ed-bda9-2c5e901a9000&id=292841&aadredir=1&whr=outlook.sg&CBCXT=out&lw=1&fl=dob%2cflname%2cwld&cobrandid=90015')\n",
    "    type_into('//*[@type=\"email\"]', account + '[enter]')\n",
    "    type_into('//*[@name=\"passwd\"]', password + '[enter]')\n",
    "    \n",
    "def search_keyword(keyword):\n",
    "    clear_button  = '//button[@aria-label=\"Exit Search\"]'\n",
    "    search_button = '//button[@aria-label=\"Search\"]'\n",
    "    if t.present(clear_button):\n",
    "        click(clear_button)\n",
    "    type_into('//input[contains(@aria-label, \"Search\")]', keyword )\n",
    "    click(search_button)\n",
    "\n",
    "def extract_outlook_email_headline(limit=100):\n",
    "    list_item = []\n",
    "    listbox_xpath = '//div[@role=\"listbox\"]'\n",
    "    if limit == 0 :\n",
    "        limit = 1000\n",
    "    for i in range(1, limit+1):\n",
    "        item_xpath = '(' + listbox_xpath + '//div[@role=\"option\"])[{}]'.format(i)\n",
    "        if not present(item_xpath):\n",
    "            print('email {} is not present'.format(i))\n",
    "            break\n",
    "        email_id = t.read(item_xpath + '/@data-convid')\n",
    "        email_headline = t.read(item_xpath + '/@aria-label')\n",
    "        list_item.append((email_id, email_headline))\n",
    "    return np.array(list_item)\n",
    "    \n",
    "def extract_google_email_headline(limit=100):\n",
    "    list_item = []\n",
    "    listbox_xpath = '(//table[@class=\"F cf zt\"])[2]'\n",
    "    if limit == 0 :\n",
    "        limit = 1000\n",
    "    i = 1\n",
    "    while len(list_item) < limit:\n",
    "        item_xpath = '(' + listbox_xpath + '//tr)[{}]//td[@class=\"xY a4W\"]'.format(i)\n",
    "        if not present(item_xpath):\n",
    "            older_button = '//div[@data-tooltip=\"Older\"]'\n",
    "            click(older_button)\n",
    "            i = 1\n",
    "            continue\n",
    "        email_id = \"\"\n",
    "        email_headline = clean_raw_text(read(item_xpath))\n",
    "        list_item.append((email_id, email_headline))\n",
    "        i += 1\n",
    "    return np.array(list_item)\n",
    "\n",
    "\n",
    "def get_email_content(i):\n",
    "    listbox_xpath = '//div[@role=\"listbox\"]'\n",
    "    item_xpath = '(' + listbox_xpath + '//div[@role=\"option\"])[{}]'.format(i)\n",
    "    click(item_xpath)\n",
    "    clean_html_text = clean_raw_text(read('//div[@class=\"wide-content-host\"]'))\n",
    "    click('//button[@aria-label=\"More mail actions\"]')\n",
    "    click('//button[@name=\"Mark as unread\"]')\n",
    "    t.wait(1)\n",
    "    click('//button[@aria-label=\"Close\"]')\n",
    "    return clean_html_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.init(visual_automation = False, chrome_browser = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RPA][ERROR] - cannot find //*[@type=\"email\"]\n",
      "[RPA][ERROR] - cannot find //*[@name=\"passwd\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login_outlook(email_account, email_pwd)\n",
    "t.wait(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"' Getting too much email? Unsubscribe | Manage subscriptionsJSJobStreet. com Singapore Sat 4/18/2020 10:58 AM You . View in web browser. Dear raymond,. We understand that it can be overwhelming for candidates to deal with so much uncertainty in the world right now. That is why we have put together a COVID-19 resource hub to connect you with. Jobs. Immediate job vacancies to supplement income. Featured Employers. Companies hiring amid the COVID-19. Resources. Insights and tips to help you navigate through this challenging period. Remember, “Tough times don’t last, but tough people do”. Together, we can overcome!. #SGUnitedJobs. Take me there. Sincerely,. JobStreet Singapore. Connect with JobStreet. Private Policy . Terms & Conditions. JobStreet Singapore. 10 Anson Road, #05-20 International Plaza, Singapore 079903. * This is not an unsolicited message. To . unsubscribe please . click here'\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_email_content(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyword('order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook_emails = extract_outlook_email_headline(limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RPA][ERROR] - cannot find ((//table[@class=\"F cf zt\"])[2]//tr)[51]//td[@class=\"xY a4W\"]\n",
      "[RPA][ERROR] - cannot find ((//table[@class=\"F cf zt\"])[2]//tr)[51]//td[@class=\"xY a4W\"]\n"
     ]
    }
   ],
   "source": [
    "gmails = extract_google_email_headline(limit=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "emails = outlook_emails\n",
    "\n",
    "with open('all_email_3.csv', mode='w' ,newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['id', 'label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter='|')\n",
    "\n",
    "    writer.writeheader()\n",
    "    for email in emails:\n",
    "        writer.writerow({'id': email[0], 'label': email[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import os\n",
    "java_path = \"C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/software/CRF-NER.html#Download\n",
    "# st = StanfordNERTagger('stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "st = StanfordNERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = clean_raw_text(t.read('//div[@class=\"wide-content-host\"]'))\n",
    "for sent in nltk.sent_tokenize(input_text):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template extraction based on text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_status_words = [\n",
    "    'your order is being processed.',\n",
    "    'your order has been confirmed.',\n",
    "    'your order has been shipped.',\n",
    "    '_ORG_ has received your order.',\n",
    "    'your order has been ackowledged by _ORG_',\n",
    "    'your order has been ackowledged by _PERSON_',\n",
    "    'we have received your order.',\n",
    "    'we are glad to inform you that your order _GPE_ has been fully delivered',\n",
    "    'Your Order _GPE_ has been placed on _DATE_.',\n",
    "    'Item from your order _GPE_ has been Shipped',\n",
    "    'Item(s) from your order _GPE_ has been Shipped',\n",
    "    'Your order will be shipped in _NUMBER_ business days',\n",
    "    'your package is on the way',\n",
    "    'your package will arrive soon',\n",
    "    'your package has shipped out'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "def get_cosine_sim(*strs): \n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)\n",
    "    \n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "\n",
    "def get_arg_max(sim_metrics):\n",
    "    return np.argmax(np.amax(sim_metrics, axis=1))\n",
    "\n",
    "def get_order_number_sentence(raw_doc):\n",
    "    doc = clean_raw_text(raw_doc)\n",
    "    raw_texts_0 = nltk.sent_tokenize(doc)\n",
    "    raw_texts = []\n",
    "    for raw_text in raw_texts_0:\n",
    "        if len(raw_text.split(' ')) < 3:\n",
    "            continue\n",
    "        raw_texts.append(raw_text)\n",
    "    masked_raw_texts = []\n",
    "    for raw_text in (raw_texts):\n",
    "        masked_raw_texts.append(remove_entity_words(raw_text))\n",
    "    sim_metrics = np.zeros((len(masked_raw_texts),len(order_status_words)))\n",
    "    for i in range(len(masked_raw_texts)):\n",
    "        for j in range(len(order_status_words)):\n",
    "            text = masked_raw_texts[i]\n",
    "            sentence = order_status_words[j]\n",
    "            sim_metrics[i,j] = get_cosine_sim(text, sentence)[0,1]\n",
    "    return raw_texts[get_arg_max(sim_metrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kim's Catering Pte Ltd has acknowledged your order -.\""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_order_number_sentence(t.read('//div[@class=\"wide-content-host\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-c3b4e1c8052d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model/order_status_sample.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.load(\"model/order_status_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hh', 'bb']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('model/order_status_sample.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_str = read('(//div[contains(@class,\"showHoverActionsOnHover\")])[1]/@class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_str = read('(//div[contains(@class,\"showHoverActionsOnHover\")])[2]/@class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    t.hover('(//div[contains(@class,\"showHoverActionsOnHover\")])[{}]'.format(i+1))\n",
    "    t.wait(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'JSJobStreet. com Singapore Sat 4/18/2020 10:58 AM You . View in web browser. Dear raymond,. We understand that it can be overwhelming for candidates to deal with so much uncertainty in the world right now. That is why we have put together a COVID-19 resource hub to connect you with. Jobs. Immediate job vacancies to supplement income. Featured Employers. Companies hiring amid the COVID-19. Resources. Insights and tips to help you navigate through this challenging period. Remember, “Tough times don’t last, but tough people do”. Together, we can overcome!. #SGUnitedJobs. Take me there. Sincerely,. JobStreet Singapore. Connect with JobStreet. Private Policy . Terms & Conditions. JobStreet Singapore. 10 Anson Road, #05-20 International Plaza, Singapore 079903. * This is not an unsolicited message. To . unsubscribe please . click here'\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_email_content(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_email_unread(class_str):\n",
    "    class_n = class_str.split('showHoverActionsOnHover')\n",
    "    if len(class_n) <2:\n",
    "        return False\n",
    "    if class_n[1] != '' :\n",
    "        return True\n",
    "    return False\n",
    "is_email_unread(class_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
